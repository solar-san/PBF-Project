---
title: "Project: The Forecasting Tourism 2010 Competition"
subtitle: "EM1415"
author: "Marco Solari, 875475"
format: pdf
papersize: a4
geometry:
  - left=23mm
  - right=35.5mm
  - marginparwidth=23mm
  - top=30mm
  - bottom=30mm
toc: true
highlight-style: breeze
code-block-bg: true

mainfont: 'Atkinson Hyperlegible'
monofont: 'Fira Code'
mathfont: 'Noto Sans Math'
sansfont: RobotoSerifNormalRoman-Medium
fontsize: 9pt        

fig-align: center
fig-width: 10
fig-height: 8
number-sections: true
df-print: kable
tbl-cap-location: bottom

include-in-header: 
  text: |
    \usepackage{fvextra}
    \usepackage{listings}
    \usepackage{lstfiracode}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
    \newfontfamily\sectionfont[]{RobotoSerifNormalRoman-ExtraBold}
    \newfontfamily\subsectionfont[]{RobotoSerifNormalRoman-Medium}
    \newfontfamily\subsubsectionfont[]{RobotoSerifNormalRoman-Medium}
    \addtokomafont{section}{\sectionfont}
    \addtokomafont{subsection}{\subsectionfont}
    \addtokomafont{subsubsection}{\subsubsectionfont}
    \makeatletter
    \def\verbatim@nolig@list{}
    \makeatother

reference-location: margin 
citation-location: margin 
---

---
\newpage

# Setup and Data Loading

## Setup

```{r setup, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(
  echo = T,
  dev = "cairo_pdf"
)

libraries_list <- c(
  "tidyverse",
  "fpp3",
  "ggthemes"
  
)

lapply(
  X = libraries_list,
  FUN = require,
  character.only = TRUE
)

theme_set(
  ggthemes::theme_tufte(
    base_size = 16,
    base_family = "Atkinson Hyperlegible"
  )
)
```

## Loading Data {#sec-dataload}

```{r load data}
data_main <- readr::read_csv(
  "Data/tourism_data.csv",
  show_col_types = F
)
```

```{r checking dimensions and NA 1}
data_main %>% dim
```

```{r checkig dimensions and NA 2}
data_main %>% is.na() %>% sum
```

We are missing `r round(100*(data_main %>% is.na() %>% sum) / (data_main %>% nrow() * data_main %>% ncol()), digits = 2)`% of the observations.


## Creating `tsibble`

```{r creating tourism tsibble}
tourism_full <- data_main %>% 
  mutate(
    Year = 1965:2007
  ) %>% 
  as_tsibble(
    index = Year
  )
```

# Assignment

## Full Plot

> _Plot all the series (an advanced data visualization tool is recommended) - what type of components are visible? Are the series similar or different? Check for problems such as missing values and possible errors._

```{r melted tsibble}
tmelt <- reshape2::melt(tourism_full,id="Year")
```

```{r tmelt dim}
tmelt %>% dim()
```

`tmelt` (@tbl-tmelt) contains the _melted_ data frame, which allows us to visualize all 518 time series at once. 

```{r tmelt head}
#| label: tbl-tmelt
#| echo: false
#| tbl-cap: "Melted `tsibble` containing all time series."
tmelt %>% 
  tail(10)
```

```{r full viz train}
#| label: fig-every
#| fig-cap: "Printing a legend for 518 different series is not possible. However, color has been used only to differentiate the series and does not contain further information. Plotting the y-axis variable on the log scale was made necessary by the huge variation in the series values."
#| fig-width: 12
#| fig-height: 12
tmelt %>% 
  ggplot(
    aes(
      x = Year,
      y = value,
      colour = variable,
      group = variable
      )
    ) + 
  geom_line(
    alpha = .8
  ) + 
  scale_y_log10() +
  scale_color_viridis_d(
    option = "cividis"
  ) +
  labs(
    title = "Tourism Time Series: Everything All At Once",
    y = "Value"
  ) +
  theme(
    legend.position = "none"
    )
```

Plotting all 518 series does not allow to spot details, such as the presence of seasonal patterns. However, a general upward trend is clear; moreover, we can spot some probable outliers, that should be further investigated, and some clues about the presence of cyclicality in some of the series.

A check for `NA`s has already been made while loading data (@sec-dataload) and it showed the presence of a large number of missing values, corresponding to `r round(100*(data_main %>% is.na() %>% sum) / (data_main %>% nrow() * data_main %>% ncol()), digits = 2)` of all observations. While this is mostly to be attributed to the different starting time of the series, we can also see that some time series have missing values occurring between the starting and ending point; hence, some series present missing values _in between_ available observations. From the visualization we can definitely group different starting points: this suggests that another approach to visualisation might be successfully conducted by grouping series by their starting date.

```{r binned NAs table}
#| tbl-cap: "Missing observation grouped by time windows."
tmelt %>% 
  mutate(
    Time_Interval = cut(
      Year,
      breaks = c(1964, 1975, 1985, 1995, 2003, 2008)
    )
  ) %>% 
  group_by(Time_Interval) %>% 
  summarise(
    Available_Observations = sum(
      !is.na(value)
      )
  )
```
```{r quartiles and mean plot}
#| fig-cap: "The following plot represents the position indexes given by the mean and the quartile, computed by considering all the series as observations and discarding missing observations."
#| fig-width: 12
#| fig-height: 10
tmelt %>% 
  mutate(
    Time_Interval = cut(
      Year,
      breaks = c(1964, 1975, 1985, 1995, 2003, 2008)
    )
  ) %>%  
  group_by(Year) %>% 
  mutate(
    median_value = median(value, na.rm = T),
    mean_value = mean(value, na.rm = T),
    q_0.25 = quantile(value, probs = .25, na.rm = T),
    q_0.75 = quantile(value, probs = .75, na.rm = T)
  ) %>% 
  ggplot(
    aes(
      x = Year
      )
    ) + 
  geom_line(
    aes(
      y = mean_value,
    color = viridisLite::cividis(1, begin = 1),
    ),
    linewidth = 1,
    linetype = "dotted"
  ) + 
  geom_line(
    aes(
      y = median_value,
    color = viridisLite::cividis(1, begin = .5)
    ),
    linewidth = 1
  ) + 
  geom_line(
    aes(
      y = q_0.25,
    color = viridisLite::cividis(1, begin = .25)
    ),
    linewidth = 1
  ) + 
  geom_line(
    aes(
      y = q_0.75,
    color = viridisLite::cividis(1, begin = .75)
    ),
    linewidth = 1
  ) + 
  geom_ribbon(
    aes(
      ymin = q_0.25,
      ymax = q_0.75,
    ),
    fill = "grey95",
    alpha = .5
  ) +
  scale_y_log10()  +
  scale_color_viridis_d(
    labels = c(
      expression(q[0.25]),
      expression(q[0.50]),
      expression(q[0.75]),
      expression(mu)
      ),
    option = "cividis",
    direction = -1,
    end = .9
  ) +
  labs(
    title = "Tourism Time Series: Quartiles and Mean",
    y = "Value",
    colour = "Index"
  )
```

```{r}
#| include: false
#| eval: false
#| fig-width: 12
#| fig-height: 30
p1 <- tmelt %>% 
  mutate(
    Time_Interval = cut(
      Year,
      breaks = c(1964, 1975, 1985, 1995, 2003, 2008)
    )
  ) %>% 
  filter(
    Time_Interval == "(1964,1975]" |
    Time_Interval == "(1975,1985]"
  )  %>% 
  ggplot(
    aes(
      x = Year,
      y = value,
      colour = variable,
      group = variable
      )
    ) + 
  geom_line(
    alpha = .8
  ) + 
  scale_y_log10() +
  scale_color_viridis_d(
    option = "cividis"
  ) +
  labs(
    y = "Value"
  ) +
  theme(
    legend.position = "none"
    )
p2 <- tmelt %>% 
  mutate(
    Time_Interval = cut(
      Year,
      breaks = c(1964, 1975, 1985, 1995, 2003, 2008)
    )
  ) %>% 
  filter(
    Time_Interval == "(1985,1995]"
  )  %>% 
  ggplot(
    aes(
      x = Year,
      y = value,
      colour = variable,
      group = variable
      )
    ) + 
  geom_line(
    alpha = .8
  ) + 
  scale_y_log10() +
  scale_color_viridis_d(
    option = "cividis"
  ) +
  labs(
    y = ""
  ) +
  theme(
    legend.position = "none"
    )
p3 <- tmelt %>% 
  mutate(
    Time_Interval = cut(
      Year,
      breaks = c(1964, 1975, 1985, 1995, 2003, 2008)
    )
  ) %>% 
  filter(
    Time_Interval == "(1995,2003]"
  )  %>% 
  ggplot(
    aes(
      x = Year,
      y = value,
      colour = variable,
      group = variable
      )
    ) + 
  geom_line(
    alpha = .8
  ) + 
  scale_y_log10() +
  scale_color_viridis_d(
    option = "cividis"
  ) +
  labs(
    y = "Value"
  ) +
  theme(
    legend.position = "none"
    )
p4 <- tmelt %>% 
  mutate(
    Time_Interval = cut(
      Year,
      breaks = c(1964, 1975, 1985, 1995, 2003, 2008)
    )
  ) %>% 
  filter(
    Time_Interval == "(2003,2008]"
  )  %>% 
  ggplot(
    aes(
      x = Year,
      y = value,
      colour = variable,
      group = variable
      )
    ) + 
  geom_line(
    alpha = .8
  ) + 
  scale_y_log10() +
  scale_color_viridis_d(
    option = "cividis"
  ) +
  labs(
    y = ""
  ) +
  theme(
    legend.position = "none"
    )

p1 / p2 / p3 / p4
```

## Creating Validation Set

> _Partition the series into training and validation, so that the last 4 years are in the validation period for each series. What is the logic of such a partitioning? What is the disadvantage?_

```{r creating train and test split}
train <- tourism_full %>% 
  filter(Year < 2004)
validation <- tourism_full %>% 
  filter(Year >= 2004)
```

The logic behind partitioning the series into a _training_ and _validation_ set is to _estimate the forecasting error_: we can train a model or apply a filter to the train set and use it to assess its performance with out-of-sample data. The main disadvantage with this approach is that we are not using all the information available to train our model; moreover, we are not computing _true forecasts_, therefore the accuracy measures from the residuals will be smaller.

## Naïve Forecasts

> _Generate naïve forecasts for all series for the validation period. For each series, create forecasts with horizons of 1, 2, 3, and 4 years ahead ($F_{t+1}, F_{t+2}, F_{t+3}$, and $F_{t+4}$)._

We know that _naïve_ forecasts consist in the last observation, $\forall h$. 

$$
\tag{1}
y_{T + h \ | \ T} = y_T
$$ {#eq-naive}

It follows that we can produce the forecasts with the following code:

```{r naive forecast at 1}
naive_forecast <- train %>% 
  filter(
    Year == 2003
  ) %>% as_tibble()
```

`naive_forecast` will contain a `tsibble` with $y_t$ for all the series.

```{r checking dimensions}
naive_forecast %>% dim
```

To obtain $F_{t+1}, F_{t+2}, F_{t+3}$, and $F_{t+4}$:

```{r all naive forecasts tsibble}
naive_2004_2007 <- merge(
    x = 2003 + 1:4,
    y = naive_forecast
  ) %>% 
  mutate(
    Year = x
  ) %>% 
  select(
    -x
  ) %>% 
  as_tsibble(
    index = Year
  )
```

## Choosing Measures

> _Which measures are suitable if we plan to combine the results for the 518 series? Consider MAE, Average error, MAPE and RMSE._

## Computing MAPE

> _For each series, compute MAPE of the naive forecasts once for the training period and once for the validation period._

### Training Period:

It follows from the definition of _naïve forecasts_ (@eq-naive) that what is needed to compute the MAPE for all the training dataset is just a modified version of it in which all rows change their position by 1.
The last forecast can be added by binding it to maintain the same dimensions and have a full forecast matrix.

```{r training NAIVE forecasts}
train_forecasts <- train %>% 
  select(-Year) %>% 
  as_tibble()
first_forecast <- rep(
  NA,
  518
)
naive_train <- rbind(
  c(
    rep(
      NA,
      dim(train_forecasts)[2]
      )
  ),
  train_forecasts[1:(dim(train_forecasts)[1] -1), ]
)
```

```{r phat training naive}
uhat_full <- naive_train - train
p_uhat_full <- uhat_full/train
```

```{r NAIVE training MAPE}
mape_full <-  100*apply(
    X = p_uhat_full %>% select(-Year),
    FUN = mean,
    MARGIN = 2,
    na.rm = T
  )
```

```{r dim check training MAPE array}
mape_full %>% length
```

```{r prettyprint training MAPE}
#| tbl-cap: "Naïve forecasts training MAPE for the first 10 time series from 1998 to 2003."
mape_full[1:10] %>% 
  round(., digits = 2) %>% 
  t() %>% 
  as_tibble() %>% 
  tail()
```

### Validation Period:

```{r computing validation residuals}
uhat_validation <- naive_2004_2007 - validation
p_uhat <- uhat_validation/validation
```

```{r mape_validation}
mape_validation <-  100*apply(
    X = p_uhat %>% select(-Year),
    FUN = mean,
    MARGIN = 2,
    na.rm = T
  )
```

```{r dim check validation MAPE array}
mape_validation %>% length
```

This is the MAPE for the first 10 series.

```{r prettyprint validation MAPE}
#| tbl-cap: "Naïve forecasts validation MAPE for the first 10 time series for the years 2004, 2005, 2006, 2007."
mape_validation[1:10] %>% 
  round(., digits = 2) %>% 
  t() %>% 
  as_tibble()
```

## Computing MASE

> _The performance measure used in the competition is Mean Absolute Scaled Error (MASE). Explain the advantage of MASE and compute the training and validation MASE for the naive forecasts._

## MAPE Pairs

> _Create a scatter plot of the MAPE pairs, with the training MAPE on the x-axis and the validation MAPE on the y-axis. Create a similar scatter plot for the MASE pairs. Now examine both plots. What do we learn? How does performance differ between the training and validation periods? How does performance range across series?_

```{r MAPE scatterplot}
#| fig-width: 12
#| fig-height: 10
#| fig-cap: "Scatterplot of training and validation MAPE pairs: on both axis, the distribution of values. The time series have been colored using the same mapping seen in @fig-every."
ggplot(
  data = tibble(
    Training_MAPE = mape_full,
    Validation_MAPE = mape_validation,
    Series_Identifier = names(mape_full)
  ),
  aes(
    x = Training_MAPE,
    y = Validation_MAPE,
  color = Series_Identifier
  ),
) + geom_point(
  alpha = .8
) +
  geom_rug() +
  labs(
    title = "Training and Validation MAPE pairs, colored by series",
    x = "Training MAPE",
    y = "Validation MAPE"
  ) + 
  scale_color_viridis_d(
    option = "cividis"
  ) +
  ggthemes::theme_tufte(
    base_size = 16,
    base_family = "Atkinson Hyperlegible",
    ticks = F
  ) +
  theme(
    legend.position = "none"
  )
```


## Ensemble Methods

> _The competition winner, Lee Baker, used an ensemble of three methods:_
>
>  - _Naive forecasts multiplied by a constant trend_^[Global/local trend: ”globally tourism has grown ”at a rate of 6% annually.”].
>  - Linear regression.
>  - Exponentially-weighted linear regression.

### a. _Write the exact formula used for generating the first method, in the form $F_{t+k} = ... (k = 1, 2, 3, 4)$,_


### b. _What is the rational behind multiplying the naive forecasts by a constant?_^[Hint: think empirical and domain knowledge.]

### c. _What should be the dependent variable and the predictors in a linear regression model for this data? Explain._.

### d. _Fit the linear regression model to the first five series and compute forecast errors for the validation period._

```{r select first 5 series}
train_subset <- train %>% 
  select(
    Y1,
    Y2,
    Y3,
    Y4,
    Y5
  )
```

### e. _Before choosing a linear regression, the winner described the following process:_

”I examined fitting a polynomial line to the data and using the line to predict future values. I tried using first through fifth order polynomials to find that the lowest MASE was obtained using a first order polynomial (simple regression line). This best fit line was used to predict future values. I also kept the $R^2$ value of the fit for use in blending the results of the prediction.”

> _What are two flaws in this approach?_

### f. _If we were to consider exponential smoothing, what particular type(s) of exponential smoothing are reasonable candidates?_

### g. _The winner concludes with possible improvements one being ”an investigation into how to come up with a blending ensemble method that doesn’t use much manual twerking would also be of benefit”. Can you suggest methods or an approach that would lead to easier automation of the ensemble step?_

### h. _The competition focused on minimizing the average MAPE of the next four values across all 518 series. How does this goal differ from goals encountered in practice when considering tourism demand? Which steps in the forecasting process would likely be different in a real-life tourism forecasting scenario?_