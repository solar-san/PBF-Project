---
title: "Project: The Forecasting Tourism 2010 Competition"
subtitle: "EM1415"
author: "Marco Solari, 875475"
format: pdf
papersize: a4
geometry:
  - left=23mm
  - right=35.5mm
  - marginparwidth=23mm
  - top=30mm
  - bottom=30mm
toc: true
highlight-style: breeze
code-block-bg: true

mainfont: 'Atkinson Hyperlegible'
monofont: 'Fira Code'
mathfont: 'Noto Sans Math'
sansfont: RobotoSerifNormalRoman-Medium
fontsize: 9pt        

fig-align: center
fig-width: 10
fig-height: 8
number-sections: true
df-print: kable
cap-location: margin

include-in-header: 
  text: |
    \usepackage{fvextra}
    \usepackage{listings}
    \usepackage{lstfiracode}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
    \newfontfamily\sectionfont[]{RobotoSerifNormalRoman-ExtraBold}
    \newfontfamily\subsectionfont[]{RobotoSerifNormalRoman-Medium}
    \newfontfamily\subsubsectionfont[]{RobotoSerifNormalRoman-Medium}
    \addtokomafont{section}{\sectionfont}
    \addtokomafont{subsection}{\subsectionfont}
    \addtokomafont{subsubsection}{\subsubsectionfont}
    \makeatletter
    \def\verbatim@nolig@list{}
    \makeatother

reference-location: margin 
citation-location: margin 
---

------------------------------------------------------------------------

\newpage

# Setup and Data Loading

## Setup

```{r setup}
#| results: "hide"
#| warning: false
#| message: false 
knitr::opts_chunk$set(
  echo = T,
  dev = "cairo_pdf"
)

libraries_list <- c(
  "tidyverse",
  "fpp3",
  "ggthemes"
  
)

lapply(
  X = libraries_list,
  FUN = require,
  character.only = TRUE
)

theme_set(
  ggthemes::theme_tufte(
    base_size = 16,
    base_family = "Atkinson Hyperlegible"
  )
)
```

## Loading Data {#sec-dataload}

```{r load data}
data_main <- readr::read_csv(
  "Data/tourism_data.csv",
  show_col_types = F
)
```

```{r checking dimensions and NA 1}
data_main %>% 
  dim
```

```{r checkig dimensions and NA 2}
data_main %>% 
  is.na() %>% 
  sum
```

We are missing `r round(100*(data_main %>% is.na() %>% sum) / (data_main %>% nrow() * data_main %>% ncol()), digits = 2)`% of the observations.

## Creating `tsibble`

```{r creating tourism tsibble}
tourism_full <- data_main %>% 
  mutate(
    Year = 1965:2007
  ) %>% 
  as_tsibble(
    index = Year
  )
```

`tmelt` (@tbl-tmelt) contains the *melted* data frame, which allows us to apply the tidy forecasting workflow all 518 time series at once. Its main variables are:

- `index`: `Year`, as in the original data frame.
- `key`: `Identifier`, a new categorical variable allowing us to transform the data frame the tidy format; it consists in a set of _labels_ that identify each time series.
- `value`: the $Y_t$ value for each time series.


```{r melted tsibble}
tmelt <- reshape2::melt(
  tourism_full, 
  id = "Year",
  variable.name = "Identifier",
  value.name = "Value"
  ) %>% 
  as_tsibble(
    index = "Year",
    key = "Identifier"
  )
```

```{r tmelt dim}
tmelt %>% 
  dim()
```

```{r tmelt head}
#| label: tbl-tmelt
#| echo: false
#| tbl-cap: "Excerpt of melted `tsibble` containing all time series."
tmelt %>% 
  tail(10)
```

# Assignment

## Full Plot

In all the subsequent plots, a $log_{10}$ transformation has been employed exclusively for representing the time series on the y-axis. This adjustment becomes necessary since the original data range^[$10^9$, shown in @tbl-range.] does not permit a clear and meaningful visualization of the series when plotted together.

```{r}
#| label: tbl-range
#| tbl-cap: "Range of Tourism Time Series"
tmelt %>% 
  reframe(
    "Range" =  range(
      Value, 
      na.rm = T, 
      finite = T
      )
    ) %>% 
  mutate(
    "Y" = c(
      "min", 
      "max"
      ),
    .before = "Range"
    )
```

### Everything, Everywhere, All At Once

> *Plot all the series (an advanced data visualization tool is recommended) - what type of components are visible? Are the series similar or different? Check for problems such as missing values and possible errors.*

```{r full viz train}
#| label: fig-every
#| fig-cap: "Printing a legend for 518 different series is not a viable option. However, color has been used only to differentiate the series and does not contain further information. Plotting the y-axis variable on the log scale was made necessary by the huge variation in the series values."
#| fig-full: 12
#| fig-height: 12
#| warning: false
tmelt %>% 
  ggplot(
    aes(
      x = Year,
      y = Value,
      colour = Identifier,
      group = Identifier
      )
    ) + 
  geom_line(
    alpha = .8
  ) + 
  scale_y_log10() +
  scale_color_viridis_d(
    option = "cividis"
  ) +
  labs(
    title = "Tourism Time Series: Everything All At Once",
    y = expression(log[10](Value))
  ) +
  theme(
    legend.position = "none",
    plot.margin = margin(
      1,
      1,
      3,
      1
    )
  )
```

While plotting all 518 series simultaneously may hinder the clear identification of specific details, a distinct overall upward trend is discernible. Additionally, noteworthy outliers emerge, warranting further investigation. Furthermore, subtle indications of cyclicality in certain series can be observed.

A check for `NA`s has already been made while loading data (@sec-dataload) and it showed the presence of a large number of missing values, corresponding to `r round(100*(data_main %>% is.na() %>% sum) / (data_main %>% nrow() * data_main %>% ncol()), digits = 2)`% of all observations. Clearly, this can be attributed to the distinct initial timestamps of the series. It is evident that we can categorize these series based on their respective starting years, indicating that an alternative visualization approach could be effectively implemented through this grouping method (@fig-tsgrouped).

```{r NAs by year table}
#| tbl-cap: "Missing observation by year: the presence of missing observations is related to the scarcity of long-run time series."
tmelt %>% 
  summarise(
    "Available Observations" = sum(
      !is.na(Value)
      )
  )
```

The set of complete time series starts in 2001.

### Plotting Series By Starting Year

Arranging the series chronologically by their starting year not only aids in evaluating their variability but also amplifies clarity. 

This grouping stresses the already noted upward trend, with the exception of most series kickstarting in 2001 (top-left subplot of @fig-tsgrouped). Another notable group of outlier can be seen in the subplot titled 18^[Time series starting in `r 2007 - 18`.]: in this group we can clearly spot a cluster of series in which the upward trend is inverted.

```{r ts plot grouped by length}
#| label: fig-tsgrouped
#| fig-cap: "All series have been grouped by starting year and plotted to achieve more clarity. Each subtitle represent the number of periods for each subset. The same color mapping of @fig-every has been used to differentiate the series."
#| fig-width: 12
#| fig-height: 18
#| warning: false
tmelt %>% 
  group_by(
    Identifier
    ) %>% 
  mutate(
    series_length = 43 -  Value %>% is.na %>% sum
    ) %>% 
  ungroup() %>% 
  arrange(
    desc(
      series_length
      )
    ) %>% 
  mutate(
    series_length = as_factor(series_length)
    ) %>% 
  ggplot(
    aes(
      x = Year
      )
  ) +
  facet_wrap(
    ~series_length,
    nrow = 6,
    ncol = 3,
    scales = "free"
  ) +
  geom_line(
    aes(
      y = Value,
      color = Identifier
    )
  ) +
  labs(
    title = "Tourism Time Series By Starting Year",
    y = expression(log[10](Value))
  ) +
  scale_y_log10() +
  scale_color_viridis_d(
    option = "cividis"
  ) +
  theme(
    legend.position = "none"
  )
```

## Creating Validation Set

> *Partition the series into training and validation, so that the last 4 years are in the validation period for each series. What is the logic of such a partitioning? What is the disadvantage?*

```{r creating train and test split}
train <- tmelt %>% 
  filter(
    Year < 2004
    )
validation <- tmelt %>% 
  filter(
    Year >= 2004
    )
```

```{r}
validation %>% 
  head(8)
```

The logic behind partitioning the series into a *training* and *validation* set is to *estimate the forecasting error*: we can train a model or apply a filter to the train set and use it to assess its performance with out-of-sample data. The main disadvantage with this approach is that we are not using all the information available to train our model; moreover, we are not computing *true forecasts*, therefore the accuracy measures from the residuals will be smaller.

## Naïve Forecasts

> *Generate naïve forecasts for all series for the validation period. For each series, create forecasts with horizons of 1, 2, 3, and 4 years ahead (*$F_{t+1}, F_{t+2}, F_{t+3}$, and $F_{t+4}$).

We can produce the forecasts by applying @eq-naive:

$$
\tag{1}
y_{T + h \ | \ T} = y_T
$$ {#eq-naive}

First of all, initializing a naïve model will allow us to use `R` to compute both point forecasts and prediction intervals:

```{r validation NAIVE forecasts}
naive_model <- train %>% 
  na.omit() %>% 
  model(NAIVE(Value))
```

`naive_model` will contain a `mable` for all the series, to be used to compute both _training_ and _validation_ errors.

To obtain $F_{t+1}, F_{t+2}, F_{t+3}$, and $F_{t+4}$:

```{r NAIVE validation forecasts fable}
naive_fc <- 
  naive_model %>% 
  forecast(
    h = 4
  )
```

```{r NAIVE validation forecasts fable tail}
naive_fc %>% 
  tail(20)
```

## Choosing Measures

> *Which measures are suitable if we plan to combine the results for the 518 series? Consider MAE, Average error, MAPE and RMSE.*

When combining forecasting results for multiple time series it is crucial to account for the scale and potential variations across the series. The choice of measures can impact the overall assessment of forecasting accuracy. 

The Mean Absolute Error (MAE) is a suitable measure to quantify the average absolute errors across all series without considering the direction of the errors. It provides a straightforward indication of the average magnitude of forecasting errors. 

The Average Error^[Defined as the mean of all individual errors.] can complement the MAE by providing a simple measure of the overall bias in the forecasting. However, it does not consider the direction of errors and might not be suitable if positive and negative errors can cancel each other out. 

The Mean Absolute Percentage Error (MAPE) is suitable to evaluate the forecasting accuracy in percentage terms, which can be particularly useful when dealing with series of different scales. However, it is sensitive to series with small actual values. 

Last but not least, the Root Mean Squared Error (RMSE) is suitable to penalize larger errors more heavily^[Outliers might therefore skew its measurement.]. It provides a balance between considering both large and small errors; like MAE, it doesn't consider the direction of errors.

Having a very wide range of values, as seen in @tbl-range, the MAPE is the candidate for the most useful error measure in this setting, to ensure consistency when evaluating the forecasting error across different scaled series.

## Computing MAPE

> *For each series, compute MAPE of the naive forecasts once for the training period and once for the validation period.*

### Training Period:

```{r errors training}
errors_training <- naive_model %>% 
  accuracy()
```

This is the training MAPE for the first 10 series:

```{r mape training}
errors_training %>% 
  select(
    Identifier,
    .type,
    MAPE
  ) %>% 
  head(10)
```

### Validation Period:

```{r errors validation}
errors_validation <- 
  accuracy(
    naive_fc,
    validation
  )
```

This is the validation MAPE for the first 10 series:

```{r tbl validation MAPE}
errors_validation %>% 
   select(
    Identifier,
    .type,
    MAPE
  ) %>% 
  head(10)
```

### Comparison Table

```{r prettyprint MAPEs comparison table}
#| tbl-cap: "Naïve forecasts training and validation MAPEs for the first 10 time series."
#| warning: false
MAPE_comparison <- bind_rows(
errors_training[1:10, ] %>% 
  select(MAPE) %>% 
  round(., digits = 2) %>% 
  t() %>% 
  as_tibble() %>% 
  mutate(
    Set = "Training",
    .before = V1
  ) %>% 
  tail(),
errors_validation[1:10, ] %>% 
  select(MAPE) %>% 
  round(., digits = 2) %>% 
  t() %>% 
  as_tibble() %>% 
  mutate(
    Set = "Validation"
  )
) 

colnames(MAPE_comparison) <- c("Set", errors_training$Identifier %>% as.character() %>% unique %>% head(10))

MAPE_comparison
```

## Computing MASE

> *The performance measure used in the competition is Mean Absolute Scaled Error (MASE). Explain the advantage of MASE and compute the training and validation MASE for the naive forecasts.*

The Mean Absolute Scaled Error (MASE) serves as a robust performance metric thanks to its scale-independence, a quality that renders it well-suited for the comparative assessment of forecast accuracy across diverse time series characterized by differing scales and magnitudes, such as in this dataset, accounting for the inherent scale differences among time series.
scaling the errors based on the training MAE from a simple forecast method.

It is an alternative to _percentage errors_ such as the MAPE. For a non-seasonal time series, a useful way to define a scaled error uses naïve forecasts: because the numerator and denominator both involve values on the scale of the original data, is independent of the scale of the data.

### Training Period:

```{r tbl training MASE}
#| tbl-cap: "Training MASE for the first 10 series."
errors_training %>% 
  select(
    Identifier,
    .type,
    MASE
  ) %>% 
  head(10)
```

Since MASE gives an indication of effectiveness of forecasting algorithm with respect to a naïve forecast, its value is greater than one 1 indicates the algorithm is performing poorly compared to the naïve forecast, and vice-versa: hence, since we have been computing the naïve MASE of in-sample data, it is equal to 1 for all time series in our training dataset.

### Validation Period:

```{r validation errors}
errors_validation <- 
  accuracy(
    naive_fc,
    tmelt
  )
```

This is the validation MASE for the first 10 series: 

```{r tbl validation MASE}
#| tbl-cap: "Validation MASE for the first 10 series."
#| warning: false
errors_validation %>% 
   select(
    Identifier,
    .type,
    MASE
  ) %>% 
  head(10)
```

### Comparison Table:

```{r prettyprint MASEs comparison table}
#| tbl-cap: "Naïve forecasts training and validation MASEs for the first 10 time series."
#| warning: false
MASE_comparison <- bind_rows(
errors_training[1:10, ] %>% 
  select(MASE) %>% 
  round(., digits = 2) %>% 
  t() %>% 
  as_tibble() %>% 
  mutate(
    Set = "Training",
    .before = V1
  ) %>% 
  tail(),
errors_validation[1:10, ] %>% 
  select(MASE) %>% 
  round(., digits = 2) %>% 
  t() %>% 
  as_tibble() %>% 
  mutate(
    Set = "Validation"
  )
) 

colnames(MASE_comparison) <- c("Set", errors_training$Identifier %>% as.character() %>% unique %>% head(10))

MASE_comparison
```

## MAPE & MASE Pairs

> *Create a scatter plot of the MAPE pairs, with the training MAPE on the x-axis and the validation MAPE on the y-axis. Create a similar scatter plot for the MASE pairs. Now examine both plots. What do we learn? How does performance differ between the training and validation periods? How does performance range across series?*

```{r MAPE pairs tibble}
#| echo: false
MAPE_pairs <- tibble(
    Series_Identifier = errors_training$Identifier,
    Training_MAPE = errors_training$MAPE,
    Validation_MAPE = errors_validation$MAPE,
  )
```


```{r MAPE scatterplot}
#| fig-width: 12
#| fig-height: 9
#| fig-cap: "Scatterplot of training and validation MAPE pairs: on both axis, the distribution of values. The time series have been colored using the same mapping seen in @fig-every."
ggplot(
  data = MAPE_pairs, 
  aes(
    x = Training_MAPE,
    y = Validation_MAPE,
    color = Series_Identifier
  ),
) + geom_point(
  alpha = .8
) +
  geom_rug() +
  labs(
    title = "Training and Validation MAPE pairs, colored by series",
    x = "Training MAPE",
    y = "Validation MAPE"
  ) + 
  scale_color_viridis_d(
    option = "cividis"
  ) +
  ggthemes::theme_tufte(
    base_size = 16,
    base_family = "Atkinson Hyperlegible",
    ticks = F
  ) +
  theme(
    legend.position = "none"
  )
```

```{r MASE pairs tibble}
#| echo: false
MASE_pairs <- 
   tibble(
    Training_MASE = errors_training$MASE,
    Validation_MASE = errors_validation$MASE,
    Series_Identifier = errors_training$Identifier
  )
```

```{r MASE scatterplot}
#| fig-width: 12
#| fig-height: 9
#| fig-cap: "Scatterplot of training and validation MASE pairs: on both axis, the distribution of values. The time series have been colored using the same mapping seen in @fig-every."
#| label: fig-mase
ggplot(
  data =
    MASE_pairs,
  aes(
    x = Training_MASE,
    y = Validation_MASE,
  color = Series_Identifier
  ),
) + geom_point(
) +
  geom_rug() +
  labs(
    title = "Training and Validation MASE pairs, colored by series",
    x = "Training MASE",
    y = "Validation MASE"
  ) + 
  scale_color_viridis_d(
    option = "cividis"
  ) +
  ggthemes::theme_tufte(
    base_size = 16,
    base_family = "Atkinson Hyperlegible",
    ticks = F
  ) +
  theme(
    legend.position = "none"
  )
```

We can add some jitter to better visualise the points:

```{r jittered MASE pairs}
#| fig-width: 12
#| fig-height: 9
#| fig-cap: "Jittered scatterplot of all MASE pairs, as in @fig-mase, with jittering."
ggplot(
  data =
    MASE_pairs,
  aes(
    x = Training_MASE,
    y = Validation_MASE,
  color = Series_Identifier
  ),
) + geom_jitter() +
  geom_rug() +
  labs(
    title = "Training and Validation MASE pairs, colored by series",
    x = "Training MASE",
    y = "Validation MASE"
  ) + 
  scale_color_viridis_d(
    option = "cividis"
  ) +
  ggthemes::theme_tufte(
    base_size = 16,
    base_family = "Atkinson Hyperlegible",
    ticks = F
  ) +
  theme(
    legend.position = "none",
    axis.text.x = element_blank()
  )
```

## Ensemble Methods

> *The competition winner, Lee Baker, used an ensemble of three methods:*
>
> -   *Naive forecasts multiplied by a constant trend*[^1].
> -   Linear regression.
> -   Exponentially-weighted linear regression.

[^1]: Global/local trend: "globally tourism has grown"at a rate of 6% annually."

### a. *Write the exact formula used for generating the first method, in the form* $F_{t+k} = ...$, where $k = 1, 2, 3, 4)$, {.unnumbered}

### b. *What is the rational behind multiplying the naive forecasts by a constant?*[^2] {.unnumbered}

[^2]: Hint: think empirical and domain knowledge.

### c. *What should be the dependent variable and the predictors in a linear regression model for this data? Explain.*. {.unnumbered}

### d. *Fit the linear regression model to the first five series and compute forecast errors for the validation period.* {.unnumbered}

```{r select first 5 series}

```

### e. *Before choosing a linear regression, the winner described the following process:* {.unnumbered}

"I examined fitting a polynomial line to the data and using the line to predict future values. I tried using first through fifth order polynomials to find that the lowest MASE was obtained using a first order polynomial (simple regression line). This best fit line was used to predict future values. I also kept the $R^2$ value of the fit for use in blending the results of the prediction."

> *What are two flaws in this approach?*

### f. *If we were to consider exponential smoothing, what particular type(s) of exponential smoothing are reasonable candidates?* {.unnumbered}

### g. *The winner concludes with possible improvements one being "an investigation into how to come up with a blending ensemble method that doesn't use much manual twerking would also be of benefit". Can you suggest methods or an approach that would lead to easier automation of the ensemble step?* {.unnumbered}

### h. *The competition focused on minimizing the average MAPE of the next four values across all 518 series. How does this goal differ from goals encountered in practice when considering tourism demand? Which steps in the forecasting process would likely be different in a real-life tourism forecasting scenario?* {.unnumbered}
